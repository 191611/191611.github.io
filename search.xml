<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文解读：《Learning representations by back-propagating errors》</title>
    <url>/2025/01/22/3080401553/</url>
    <content><![CDATA[<h1 id="Learning-representations-by-back-propagating-errors"><a href="#Learning-representations-by-back-propagating-errors" class="headerlink" title="Learning representations by back-propagating errors"></a>Learning representations by back-propagating errors</h1><h2 id="通过误差反向传播学习表示"><a href="#通过误差反向传播学习表示" class="headerlink" title="通过误差反向传播学习表示"></a>通过误差反向传播学习表示</h2><blockquote>
<p>原论文作者：</p>
<p>David E. Rumelhart Institute for Cognitive Science, University of California, USA</p>
<p>Ronald J. Williams Institute for Cognitive Science, University of California, USA</p>
<p>Geoffrey E. Hinton Department of Computer Science, Carnegie-Mellon University, USA</p>
</blockquote>
<center><img src="https://raw.githubusercontent.com/191611/images/main/img/cover.png" width="75%" /></center>

<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>论文提出了一种新的学习过程：反向传播（Back-Propagation，BP）。BP 算法的主要过程是通过误差反向传播不断地调整模型的权值，从而最小化输出和真实值之间的差异。通过调整模型权重，模型的隐藏层单元能够表示任务数据中的特征，并捕捉数据的规律。</p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h2><h3 id="2-1-神经元模型的提出"><a href="#2-1-神经元模型的提出" class="headerlink" title="2.1 神经元模型的提出"></a>2.1 神经元模型的提出</h3><p>神经元模型是沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮兹（Walter Pitts）所提出的。该模型可简单描述为：将二进制输入值加起来，并在其和大于一个阈值时输出 1，否则输出 0，可用来模拟基本的或/与/非逻辑函数。</p>
<h3 id="2-2-感知机的提出"><a href="#2-2-感知机的提出" class="headerlink" title="2.2 感知机的提出"></a>2.2 感知机的提出</h3><p>感知机由 Frank Rosenblatt 在 1957 年提出。单层感知机是一个最简单的线性二分类模型，只有输入层和输出层，并不能产生新的特征。对于感知机和神经网络的区别不能仅仅从网络上区别。在上世纪，感知机的输入是一组二进制输入值（从仿生的角度来说就是附近的神经元给的电信号，0 代表没有信号、未放电；1 代表激活，放电），将每个输入值乘以一个连续值权重（每个附近神经元的突触强度），并设立一个阈值（激活函数），如果这些加权输入值的和超过这个阈值，就输出 1，否则输出 0。（可以发现，输入层的形式并不是现在我们常用的“特征”，而是更接近 One-Hot 编码，是二进制的）。</p>
<p>此外，唐纳德·赫布（Donald Hebb）提出了一个出人意料并影响深远的想法，该想法称知识和学习在大脑中的产生方式主要表现为神经元间突触的形成与变化，简要表述为赫布法则：“当细胞 A 的轴突足以接近以激发细胞 B，并反复持续地对细胞 B 放电，一些生长过程或代谢变化将发生在某一个或这两个细胞内，以致在未来 A 作为对 B 放电的细胞中的一个的效率增加。”罗森布拉特受到唐纳德·赫布（Donald Hebb）基础性工作的启发，想出一个让这种人工神经元学习的办法：</p>
<p>给定一个有输入输出实例的训练集，感知机应该“学习”一个函数：对每个例子，若感知机的输出值比实例低太多，则增加它的权重，否则若设比实例高太多，则减少它的权重。可进一步描述为：感知机从随机权重和一个训练集开始，对于训练集中一个实例的输入值，计算感知机的输出值。若感知机的输出值和实例中默认正确的输出值不同，则：（1）若输出值应该为 0 但实际为 1，则减少输入值是1的例子的权重。（2）若输出值应该为 1 但实际为 0，则增加输入值是 1 的例子的权重。</p>
<h2 id="3-挑战"><a href="#3-挑战" class="headerlink" title="3. 挑战"></a>3. 挑战</h2><p>综上所述，当前感知机的输入是 0 和 1 组成的向量，真实值是 0 或者 1（在被激活函数处理后，就变成了分类问题）代表的标量（原文中提到，也可以是向量，对应多分类）。 学习方法是，如果感知机输出的值比真实值要大（输出为 1，真实值为 0），就减少输入层中那些输入为 1 的单元所对应的权值，也就是说在计算输出层的加权和时，1 的系数就变小了，感知机输出的值也会变小。 </p>
<p>然而这种学习方法放在多层感知机，也就是有隐藏层存在的时候，就变得无效了，因为此时并不知道隐藏层的输入是多少，那么也无法调整其权重，这才为 BP 算法的出现带来了现实意义。</p>
<h2 id="4-设计"><a href="#4-设计" class="headerlink" title="4. 设计"></a>4. 设计</h2><p>如下图所示，是两层感知机的结构示意图，从下到上依次代表：输入层（input）、中间层（intermediate）、输出层（output）。分别用字母 $i, j, k$ 表示。 对于整个BP算法的参数迭代过程，主要有两个步骤：前向传播和反向传播。前向传播负责从输入 $x$ 计算得到模型输出 $y$。 而反向传播通过计算能够得到损失函数 $E$ 关于网络中所有参数的偏导数，通过修改所有参数值，令其向负梯度方向不断迭代，就能使损失函数 $E$ 不断向函数值减小的方向变化，从而达到极小化损失函数的目的。</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/%E5%9B%BE%E7%89%87%201.jpg" width="50%" />
$ $
图 1. 3输入3输出的3层感知机示意图
</center>

<h3 id="4-1-前向传播"><a href="#4-1-前向传播" class="headerlink" title="4.1 前向传播"></a>4.1 前向传播</h3><p>在前向传播过程中，主要受到链接链路权重 $w_{ji}$ 以及激活函数 $\sigma$ 的影响。其中，每一层的神经元的状态都通过公式 1 和公式 2，在神经网络中由低到高来得出。</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/%E5%9B%BE%E7%89%87%202.jpg" width="50%" />
$ $
图 2. j 层一神经元示例图
</center>

<p>如上图，为 $j$ 层的某一神经元示例，该单元的输入用黑色字体 $x_j$ 表示，$x_j$ 是上一层输出 $y_i$ 的线性函数（加权求和），$x_j$ 由公式 1 计算得到。</p>
<script type="math/tex; mode=display">
x_j=\sum_i y_i w_{ji} \tag{1}</script><p>神经元 $j$ 在收到输入 $x_j$ 后，与自身阈值 $\theta$ 判断是否输出，该阈值 $\theta$ 也可理解为 $w$ 的相反数，即：，代表单元 $j$ 的额外固定输入 1 所对应的权重。因此神经元 $j$ 的输入可直接用 $x_j$ 表示。</p>
<p>对输入 $x_j$，还需经过非线性激活函数的处理。在该论文中使用 $\mathrm{sigmoid}$，因此神经元 $j$ 最终的输出 $y_j$ 由公式 2 给出。</p>
<script type="math/tex; mode=display">
y_j=\frac{1}{1+e^{-x_j}} \tag{2}</script><p>到此完成单个神经元的正向传播计算过程，将若干个神经元按层级关系排列并相连，即可得到多层感知机网络。此外，在多层感知机中，可以包含多个中间层（隐藏层），同层连接或者从高到低的连接是禁止的，但是连接可以跳过若干中间层，由底层直接向更高层传播。</p>
<h3 id="4-2-反向传播"><a href="#4-2-反向传播" class="headerlink" title="4.2 反向传播"></a>4.2 反向传播</h3><p>在反向传播开始前，需要定义误差函数，从而对模型预测的准确程度进行描述，模型总体的误差定义为：</p>
<script type="math/tex; mode=display">
E=\frac{1}{2} \sum_c \sum_j (y_{j,c} - d_{j,c})^2 \tag{3}</script><p>其中， $c$ 是样本索引， $j$ 是输出神经元的索引， $y$ 是一个输出单元的实际状态， $d$ 是其期望状态，或真实值（ground truth），加入 $\frac{1}{2}$ 是为求导计算方便所做的改动，并不会改变其最小值点所对应的输入值 $y$ ，也就不会对模型中的参数产生影响。由于总目标是通过梯度下降法修改模型中的全部参数 $w, bias$ 从而最小化误差，因此计算误差对每一个权重的偏导数是必要的。</p>
<p>对每个输出单元，通过计算 开始反向传递过程。对某一条数据 时的公式3进行求导，并且忽略 得到输出对误差的影响：</p>
<p>(4)<br>进而也可得到输出层输入对误差的影响：</p>
<p>(5)<br>由于总目标是修改模型参数 ，因此进一步求得输出层参数对误差的影响：</p>
<p>(6)<br>但是上述过程只是对输出层求了偏导，对于中间层单元 来说，可通过以下公式得到：</p>
<p>(8)<br>但是对全连接的多层感知机来说，中间层单元 的输出会传播给上层 的全部单元，因此得到：</p>
<p>(9)<br>通过公式9的结果，参考公式4、5、6、7的链式法则，进而能够再得到该层参数 关于误差的偏导。通过反复进行上述过程，从输出层到输入层反向进行偏导计算步骤，即可求得全部权重关于误差的偏导。<br>使用该偏导数的一种方式是当每一条数据完成正向、反向传播后，都对权重进行修改，如公式10：</p>
<p>(10)<br>另一种方法是对偏导进行叠加，累积所有样本的梯度后再更新网络，也是该论文作者使用的方法。如公式11所示，此时需要对t每次加一， 是一个介于0到1之间的衰减参数，它决定当前梯度和之前的梯度对参数更新的贡献程度。</p>
<p>(11)</p>
]]></content>
      <categories>
        <category>learning</category>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Hinton</tag>
        <tag>BP算法</tag>
      </tags>
  </entry>
</search>

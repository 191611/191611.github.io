<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文解读：《Learning representations by back-propagating errors》</title>
    <url>/2025/01/22/3080401553/</url>
    <content><![CDATA[<h1 id="Learning-representations-by-back-propagating-errors"><a href="#Learning-representations-by-back-propagating-errors" class="headerlink" title="Learning representations by back-propagating errors"></a>Learning representations by back-propagating errors</h1><h2 id="通过误差反向传播学习表示"><a href="#通过误差反向传播学习表示" class="headerlink" title="通过误差反向传播学习表示"></a>通过误差反向传播学习表示</h2><center><img src="https://raw.githubusercontent.com/191611/images/main/img/cover.png" width="50%" /></center>

<blockquote>
<p>原论文作者：</p>
<p>David E. Rumelhart Institute for Cognitive Science, University of California, USA</p>
<p>Ronald J. Williams Institute for Cognitive Science, University of California, USA</p>
<p>Geoffrey E. Hinton Department of Computer Science, Carnegie-Mellon University, USA</p>
</blockquote>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>论文提出了一种新的学习过程：反向传播（Back-Propagation，BP）。BP 算法的主要过程是通过误差反向传播不断地调整模型的权值，从而最小化输出和真实值之间的差异。通过调整模型权重，模型的隐藏层单元能够表示任务数据中的特征，并捕捉数据的规律。</p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h2><h3 id="2-1-神经元模型的提出"><a href="#2-1-神经元模型的提出" class="headerlink" title="2.1 神经元模型的提出"></a>2.1 神经元模型的提出</h3><p>神经元模型是沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮兹（Walter Pitts）所提出的。该模型可简单描述为：将二进制输入值加起来，并在其和大于一个阈值时输出 1，否则输出 0，可用来模拟基本的或/与/非逻辑函数。</p>
<h3 id="2-2-感知机的提出"><a href="#2-2-感知机的提出" class="headerlink" title="2.2 感知机的提出"></a>2.2 感知机的提出</h3><p>感知机由 Frank Rosenblatt 在 1957 年提出。单层感知机是一个最简单的线性二分类模型，只有输入层和输出层，并不能产生新的特征。对于感知机和神经网络的区别不能仅仅从网络上区别。在上世纪，感知机的输入是一组二进制输入值（从仿生的角度来说就是附近的神经元给的电信号，0 代表没有信号、未放电；1 代表激活，放电），将每个输入值乘以一个连续值权重（每个附近神经元的突触强度），并设立一个阈值（激活函数），如果这些加权输入值的和超过这个阈值，就输出 1，否则输出 0。（可以发现，输入层的形式并不是现在我们常用的“特征”，而是更接近 One-Hot 编码，是二进制的）。</p>
<p>此外，唐纳德·赫布（Donald Hebb）提出了一个出人意料并影响深远的想法，该想法称知识和学习在大脑中的产生方式主要表现为神经元间突触的形成与变化，简要表述为赫布法则：“当细胞 A 的轴突足以接近以激发细胞 B，并反复持续地对细胞 B 放电，一些生长过程或代谢变化将发生在某一个或这两个细胞内，以致在未来 A 作为对 B 放电的细胞中的一个的效率增加。”罗森布拉特受到唐纳德·赫布（Donald Hebb）基础性工作的启发，想出一个让这种人工神经元学习的办法：</p>
<p>给定一个有输入输出实例的训练集，感知机应该“学习”一个函数：对每个例子，若感知机的输出值比实例低太多，则增加它的权重，否则若设比实例高太多，则减少它的权重。可进一步描述为：感知机从随机权重和一个训练集开始，对于训练集中一个实例的输入值，计算感知机的输出值。若感知机的输出值和实例中默认正确的输出值不同，则：（1）若输出值应该为 0 但实际为 1，则减少输入值是1的例子的权重。（2）若输出值应该为 1 但实际为 0，则增加输入值是 1 的例子的权重。</p>
<h2 id="3-挑战"><a href="#3-挑战" class="headerlink" title="3. 挑战"></a>3. 挑战</h2><p>综上所述，当前感知机的输入是 0 和 1 组成的向量，真实值是 0 或者 1（在被激活函数处理后，就变成了分类问题）代表的标量（原文中提到，也可以是向量，对应多分类）。 学习方法是，如果感知机输出的值比真实值要大（输出为 1，真实值为 0），就减少输入层中那些输入为 1 的单元所对应的权值，也就是说在计算输出层的加权和时，1 的系数就变小了，感知机输出的值也会变小。 </p>
<p>然而这种学习方法放在多层感知机，也就是有隐藏层存在的时候，就变得无效了，因为此时并不知道隐藏层的输入是多少，那么也无法调整其权重，这才为 BP 算法的出现带来了现实意义。</p>
<h2 id="4-设计"><a href="#4-设计" class="headerlink" title="4. 设计"></a>4. 设计</h2><p>如下图所示，是两层感知机的结构示意图，从下到上依次代表：输入层（input）、中间层（intermediate）、输出层（output）。分别用字母 $i, j, k$ 表示。 对于整个BP算法的参数迭代过程，主要有两个步骤：前向传播和反向传播。前向传播负责从输入 $x$ 计算得到模型输出 $y$。 而反向传播通过计算能够得到损失函数 $E$ 关于网络中所有参数的偏导数，通过修改所有参数值，令其向负梯度方向不断迭代，就能使损失函数 $E$ 不断向函数值减小的方向变化，从而达到极小化损失函数的目的。</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/%E5%9B%BE%E7%89%87%201.jpg" width="50%" />
$ $
图 1. 3输入3输出的3层感知机示意图
</center>

<h3 id="4-1-前向传播"><a href="#4-1-前向传播" class="headerlink" title="4.1 前向传播"></a>4.1 前向传播</h3><p>在前向传播过程中，主要受到链接链路权重 $w_{ji}$ 以及激活函数 $\sigma$ 的影响。其中，每一层的神经元的状态都通过公式 1 和公式 2，在神经网络中由低到高来得出。</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/%E5%9B%BE%E7%89%87%202.jpg" width="50%" />
$ $
图 2. j 层一神经元示例图
</center>

<p>如上图，为 $j$ 层的某一神经元示例，该单元的输入用黑色字体 $x_j$ 表示，$x_j$ 是上一层输出 $y_i$ 的线性函数（加权求和），$x_j$ 由公式 1 计算得到。</p>
<script type="math/tex; mode=display">
x_j=\sum_i y_i w_{ji} \tag{1}</script><p>神经元 $j$ 在收到输入 $x_j$ 后，与自身阈值 $\theta$ 判断是否输出，该阈值 $\theta$ 也可理解为 $w_b$ 的相反数，即：$- w_b$ ，代表单元 $j$ 的额外固定输入 1 所对应的权重。因此神经元 $j$ 的输入可直接用 $x_j$ 表示。</p>
<p>对输入 $x_j$，还需经过非线性激活函数的处理。在该论文中使用 $\mathrm{sigmoid}$，因此神经元 $j$ 最终的输出 $y_j$ 由公式 2 给出。</p>
<script type="math/tex; mode=display">
y_j=\frac{1}{1+e^{-x_j}} \tag{2}</script><p>到此完成单个神经元的正向传播计算过程，将若干个神经元按层级关系排列并相连，即可得到多层感知机网络。此外，在多层感知机中，可以包含多个中间层（隐藏层），同层连接或者从高到低的连接是禁止的，但是连接可以跳过若干中间层，由底层直接向更高层传播。</p>
<h3 id="4-2-反向传播"><a href="#4-2-反向传播" class="headerlink" title="4.2 反向传播"></a>4.2 反向传播</h3><p>在反向传播开始前，需要定义误差函数，从而对模型预测的准确程度进行描述，模型总体的误差定义为：</p>
<script type="math/tex; mode=display">
E=\frac{1}{2} \sum_c \sum_j (y_{j,c} - d_{j,c})^2 \tag{3}</script><p>其中， $c$ 是样本索引， $j$ 是输出神经元的索引， $y$ 是一个输出单元的实际状态， $d$ 是其期望状态，或真实值（ground truth），加入 $\frac{1}{2}$ 是为求导计算方便所做的改动，并不会改变其最小值点所对应的输入值 $y$ ，也就不会对模型中的参数产生影响。由于总目标是通过梯度下降法修改模型中的全部参数 $w, bias$ 从而最小化误差，因此计算误差对每一个权重的偏导数是必要的。</p>
<p>对每个输出单元，通过计算 $\frac{\partial {E} } {\partial {y} }  $ 开始反向传递过程。对某一条数据 $c$ 时的公式 3 进行求导，并且忽略 $c$ 得到输出对误差的影响：</p>
<script type="math/tex; mode=display">
\frac{\partial {E} } {\partial {y_j} } = y_j - d_j \tag{4}</script><p>进而也可得到输出层输入对误差的影响：</p>
<script type="math/tex; mode=display">
\frac{\partial {E} } {\partial {x_j} } = \frac{\partial {E} } {\partial {y_j} } \cdot \frac{\mathrm{d} {y_j} } {\mathrm{d} {x_j} } = \frac{\partial {E} } {\partial {y_j} } \cdot y_j (1-y_j)= (y_j - d_j) \cdot y_j (1-y_j) \tag{5}</script><p>由于总目标是修改模型参数 $w$，因此进一步求得输出层参数对误差的影响：</p>
<script type="math/tex; mode=display">
\frac{\partial {E} } {\partial {w_ji} } = \frac{\partial {E} } {\partial {x_j} } \cdot \frac{\partial {x_j} } {\partial{w_ji} } = \frac{\partial {E} } {\partial {x_j} } \cdot y_i = (y_j - d_j) \cdot y_j (1-y_j) \cdot y_j \tag{6}</script><p>但是上述过程只是对输出层求了偏导，对于中间层单元 $i$ 来说，可通过以下公式得到：</p>
<script type="math/tex; mode=display">
\frac{\partial {E} } {\partial {y_i} } = \frac{\partial {E} } {\partial {x_j} } \cdot \frac{\partial {x_j} } {\partial{y_i} } = \frac{\partial {E} } {\partial {x_j} } \cdot w_ji = (y_j - d_j) \cdot y_j (1-y_j) \cdot w_ji \tag{7}</script><p>但是对全连接的多层感知机来说，中间层单元 $i$ 的输出会传播给上层 $j$ 的全部单元，因此得到：</p>
<script type="math/tex; mode=display">
\frac{\partial {E} } {\partial {y_i} } = \sum_j {\frac{\partial {E} } {\partial {x_j} } \cdot w_ji }  \tag{8}</script><p>通过公式 8 的结果，参考公式 4、5、6、7 的链式法则，进而能够再得到该层参数 $w$ 关于误差的偏导。通过反复进行上述过程，从输出层到输入层反向进行偏导计算步骤，即可求得全部权重关于误差的偏导。</p>
<p>使用该偏导数的一种方式是当每一条数据完成正向、反向传播后，都对权重进行修改，如公式9：</p>
<script type="math/tex; mode=display">
\Delta w = - \frac{\partial {E} } {\partial {w} }  \tag{9}</script><p>另一种方法是对偏导进行叠加，累积所有样本的梯度后再更新网络，也是该论文作者使用的方法。如公式 10 所示，此时需要对 t 每次加 1，$\alpha$ 是一个介于 0 到 1 之间的衰减参数，它决定当前梯度和之前的梯度对参数更新的贡献程度。</p>
<script type="math/tex; mode=display">
\Delta w(t) = - \varepsilon \frac{\partial {E} } {\partial {w} } + \alpha \Delta w (t-1) \tag{10}</script><h3 id="4-3-手动推导"><a href="#4-3-手动推导" class="headerlink" title="4.3 手动推导"></a>4.3 手动推导</h3><p>以输入层，1 个隐藏层，输出层，输入 3 维输出 1 维的两层感知机为例，对反向传播算法的计算过程进行推导，与示例代码对应。</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/%E5%9B%BE%E7%89%87%203.png" width="80%" />
</center>

<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>BP 算法通过计算损失函数相对于权重的梯度来调整网络参数，从而最小化预测误差。这使得训练多层神经网络变得可行。由于 BP 网络再多层叠加后，能够有效地处理非线性问题，因此还能够解决传统线性模型难以处理的问题。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>论文解读</category>
      </categories>
      <tags>
        <tag>Hinton</tag>
        <tag>BP算法</tag>
      </tags>
  </entry>
  <entry>
    <title>论文翻译：AI-Agent中文翻译</title>
    <url>/2025/02/26/1976214198/</url>
    <content><![CDATA[<!-- 
首行缩进：&ensp;&emsp; 

-->
<h1 id="AI-Agent-翻译"><a href="#AI-Agent-翻译" class="headerlink" title="AI-Agent 翻译"></a>AI-Agent 翻译</h1><p>此版本是我在原生latex的基础上，使用GPT翻译并逐行校对，再latex编译成pdf的，为便于理解，保留了大量英文专有名词</p>
<p>错漏之处还请大家在评论区指出，谢谢 ~</p>
<object data="/files/pdf/AI-Agent中文翻译.pdf" type="application/pdf" width="100%" height="877px"></object>
]]></content>
      <categories>
        <category>学习</category>
        <category>论文翻译</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>AI</tag>
        <tag>Agent</tag>
      </tags>
  </entry>
  <entry>
    <title>模型学习：LLM学习过程记录——以 GPT-nano 为例</title>
    <url>/2025/02/26/3952906644/</url>
    <content><![CDATA[<!-- 
首行缩进：&ensp;&emsp; 

-->
<h1 id="LLM学习过程记录——以-GPT-nano-为例"><a href="#LLM学习过程记录——以-GPT-nano-为例" class="headerlink" title="LLM学习过程记录——以 GPT-nano 为例"></a>LLM学习过程记录——以 GPT-nano 为例</h1><blockquote>
<p>可视化网址：<a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a></p>
<p>GPT-nano 参数量为 85,584</p>
</blockquote>
<h2 id="问题合集"><a href="#问题合集" class="headerlink" title="问题合集"></a>问题合集</h2><h3 id="1-LayerNorm-中为什么要加入-alpha-和-beta"><a href="#1-LayerNorm-中为什么要加入-alpha-和-beta" class="headerlink" title="1. LayerNorm 中为什么要加入 $\alpha$ 和 $\beta$"></a>1. LayerNorm 中为什么要加入 $\alpha$ 和 $\beta$</h3><p>LayerNorm的公式为：</p>
<script type="math/tex; mode=display">y_i = \alpha \hat{x}_i + \beta = \alpha \left( \frac{x_i - \mu}{\sigma} \right) + \beta</script><p>其中：</p>
<ul>
<li>$x$  是输入向量（或批量数据），</li>
<li>$\mu$  是该输入的均值，</li>
<li>$\sigma$  是该输入的标准差，</li>
<li>$\alpha$  和  $\beta$  是可学习的参数，</li>
<li>$y$  是经过归一化和调整后的输出。</li>
</ul>
<h4 id="alpha-和-beta-的作用"><a href="#alpha-和-beta-的作用" class="headerlink" title="$\alpha$  和  $\beta$ 的作用"></a>$\alpha$  和  $\beta$ 的作用</h4><p>灵活性：标准化操作（通过减去均值和除以标准差）使得每一层的输入数据在分布上更加一致，减少了训练过程中的不稳定性。但归一化后的数据可能不总是适合模型的训练，因此，α 和 β 提供了恢复灵活性的手段，使得模型可以自适应地调整归一化后的数据的尺度和偏移，从而更好地拟合数据。</p>
<p>恢复信息：标准化之后，数据的均值被设置为 0，方差为 1。这对许多类型的神经网络来说可能是一个合理的起点，但一些复杂的任务可能需要特定的分布，α 和 β 让模型有能力将归一化后的数据调整为更适合任务的形式。</p>
<ul>
<li><p>$\alpha$ 控制每个特征输出的 尺度（放大或缩小），它帮助恢复原始数据的变换幅度。</p>
</li>
<li><p>$\beta$ 控制每个特征输出的 偏移，它帮助恢复原始数据的均值。</p>
</li>
</ul>
<h3 id="2-为什么除以-sqrt-d-k-能防止影响后续的-Softmax"><a href="#2-为什么除以-sqrt-d-k-能防止影响后续的-Softmax" class="headerlink" title="2. 为什么除以 $\sqrt{d_k}$ 能防止影响后续的 Softmax"></a>2. 为什么除以 $\sqrt{d_k}$ 能防止影响后续的 Softmax</h3><p>Softmax函数的作用是将一个向量转换为概率分布，公式为：</p>
<script type="math/tex; mode=display">\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</script><p>Softmax用于将点积值转换为权重，分配给不同的值。问题出现在：</p>
<ul>
<li><p>如果点积值过大，Softmax 的输出将变得非常尖锐（即其中一个元素的值接近 1，其他值接近 0），这会使得网络对某些特定的查询和键对之间的关系过于敏感，从而影响训练的稳定性。</p>
</li>
<li><p>这种过于尖锐的分布可能导致梯度消失问题，因为大部分注意力权重会集中在少数几个位置，导致其他位置的权重几乎为零，从而使得模型的学习过程非常不稳定。</p>
</li>
</ul>
<h3 id="3-为什么在-Attention-和-MLP-之前，都要加入-Norm-呢？"><a href="#3-为什么在-Attention-和-MLP-之前，都要加入-Norm-呢？" class="headerlink" title="3. 为什么在 Attention 和 MLP 之前，都要加入 Norm 呢？"></a>3. 为什么在 Attention 和 MLP 之前，都要加入 Norm 呢？</h3><ul>
<li><p>一定程度上防止梯度消失或爆炸</p>
</li>
<li><p>在训练深度神经网络时，每一层的输入分布会随着网络的训练过程而发生变化，这种变化被称为<strong>内部协变量偏移</strong>（Internal Covariate Shift）。也就是说，随着网络参数的更新，每一层的输入数据的分布可能会发生很大变化。这使得模型在训练时更难以收敛，因为每一层都必须适应不断变化的输入分布。</p>
</li>
<li><p>通过在 Attention 和 MLP 层之间加入 Layer Normalization，我们可以<strong>标准化</strong>每一层的输入（使其均值为 0，方差为 1），从而减少输入数据分布的变化，使得后续层接收到的数据更加稳定。这样，训练时每一层的输入都不会因为前一层参数的更新而发生剧烈变化，从而提升了训练的稳定性。</p>
</li>
</ul>
<h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><h3 id="1-multi-head-的多个头的结果，是拼接在一起的，而不是叠加"><a href="#1-multi-head-的多个头的结果，是拼接在一起的，而不是叠加" class="headerlink" title="1. multi-head 的多个头的结果，是拼接在一起的，而不是叠加"></a>1. multi-head 的多个头的结果，是拼接在一起的，而不是叠加</h3><p>对GPT来说，head（a=16）内的向量长度正好等于 $\frac{C}{num-heads}$ ，从而确保了将它们堆叠在一起时，能够得到原始长度 C。</p>
<h3 id="2-GPT-nano-中的-MLP-用的是-GELU-激活函数"><a href="#2-GPT-nano-中的-MLP-用的是-GELU-激活函数" class="headerlink" title="2. GPT-nano 中的 MLP 用的是 GELU 激活函数"></a>2. GPT-nano 中的 MLP 用的是 GELU 激活函数</h3><p>相比 RELU，GELU 在原点附近更平滑</p>
<center>
<img src="https://raw.githubusercontent.com/191611/images/main/img/GLUE激活函数.png" 
width="50%" />

图 1 GLUE 激活函数曲线图
</center>
]]></content>
      <categories>
        <category>学习</category>
        <category>模型学习</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
</search>
